# -*- coding: utf-8 -*-
"""Project2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B5wG19f_UyFWNbHfH8B2B8CzCsAreYTa
"""

#!curl -s https://course.fast.ai/setup/colab | bash

#!pip uninstall -y Pillow

#pip install --ignore-installed Pillow==9.0.0

#!pip install -U pillow

#!pip install image
#!pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl
#!pip3 install torchvision
import PIL

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'
import torch
import numpy as np
import matplotlib.pyplot as plt
import torch
from torch.utils.data import Subset
import time
import numpy as np
from sklearn.model_selection import train_test_split
from torch import nn, optim
import torch.nn.functional as F
import torchvision
from torchvision import datasets, transforms, models
from torchvision.transforms import Compose, ToTensor, Resize
from collections import OrderedDict
from torch.autograd import Variable
from torchvision.datasets import ImageFolder
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
from PIL import Image
from torch.optim import lr_scheduler
from copy import deepcopy
import json
import os
from os.path import exists
from random import shuffle
from numpy.lib.function_base import copy

#from google.colab import drive
#drive.mount('/content/gdrive')
#root_dir = "/content/gdrive/My Drive/"
path_breastcancer = "/home/sk9622/deep/project1/data"

#from fastai import *
#from fastai.vision import *

train_on_gpu = torch.cuda.is_available()

data_transforms = {
    'train': transforms.Compose([
        transforms.RandomRotation(30),
        transforms.RandomResizedCrop(224),
        #transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'valid': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

def train_val_dataset(dataset, val_split=0.1):
    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)
    datasets = {}
    datasets['train'] = Subset(dataset, train_idx)
    datasets['val'] = Subset(dataset, val_idx)
    return datasets

#dataset = ImageFolder(path_breastcancer, transform=data_transforms)
#print(len(dataset))
# datasets = train_val_dataset(dataset)
# print(len(datasets['train']))
# print(len(datasets['val']))
# # The original dataset is available in the Subset class
# print(datasets['train'].dataset)
# print(" The classes are :",dataset.classes)
train_dataset=ImageFolder(path_breastcancer, transform=data_transforms['train'])
val_dataset=ImageFolder(path_breastcancer, transform=data_transforms['valid'])
dataset_sizes = {'train':len(train_dataset), 'val':len(val_dataset)}
dataloaders = {'train':DataLoader(train_dataset, 32, shuffle=True, num_workers=4), 'val':DataLoader(val_dataset, 32, shuffle=True, num_workers=4)}



#dataset_sizes = {x: len(datasets[x]) for x in ['train', 'val']}
#dataloaders = {x:DataLoader(datasets[x],32, shuffle=True, num_workers=4) for x in ['train','val']}
#x,y = next(iter(dataloaders['train']))
#print(x.shape, y.shape)

# Build and train your network

# 1. Load resnet-152 pre-trained network
model = models.resnet152(pretrained=True)
# Freeze parameters so we don't backprop through them

for param in model.parameters():
    param.requires_grad = False

#Define a new, untrained feed-forward network as a classifier, using ReLU activations

# Our input_size matches the in_features of pretrained model


from collections import OrderedDict


# Creating the classifier ordered dictionary first

classifier = nn.Sequential(OrderedDict([
                          ('fc1', nn.Linear(2048, 512)),
                          ('relu', nn.ReLU()),
                          ('dropout1', nn.Dropout(p=0.5)),
                          ('fc2', nn.Linear(512, 2)),
                          ('output', nn.LogSoftmax(dim=1))
                          ]))

# Replacing the pretrained model classifier with our classifier
model.fc = classifier

#Function to train the model
def train_model(model, criterion, optimizer, scheduler, num_epochs=10):
    since = time.time()
   
    



    best_model_wts = deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(1, num_epochs+1):
        print('Epoch {}/{}'.format(epoch, num_epochs))
        print('-' * 10)

        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                scheduler.step()
                model.train()  # Set model to training mode
            else:
                model.eval()   # Set model to evaluate mode

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data.
            for inputs, labels in dataloaders[phase]:
                if torch.cuda.is_available():
                  inputs, labels = inputs.to(device='cuda'), labels.to(device='cuda')

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    _, preds = torch.max(outputs, 1)

                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / dataset_sizes[phase]
            #train_loss.append(epoch_loss)
            epoch_acc = running_corrects.double() / dataset_sizes[phase]
            #train_acc.append(epoch_acc)

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(
                phase, epoch_loss, epoch_acc))

            # deep copy the model
            if phase == 'valid' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = deepcopy(model.state_dict())

        print()

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Best valid accuracy: {:4f}'.format(best_acc))

    # load best model weights
    model.load_state_dict(best_model_wts)
    return model

# Train a model with a pre-trained network
num_epochs = 100
if torch.cuda.is_available():
    print ("Using GPU: ")
    model = model.cuda()

# NLLLoss because our output is LogSoftmax
criterion = nn.NLLLoss()
train_acc=[]
train_loss=[]

# Adam optimizer with a learning rate
optimizer = optim.RMSprop(model.fc.parameters(), lr=0.001)
#optimizer = optim.SGD(model.fc.parameters(), lr = .0006, momentum=0.9)
# Decay LR by a factor of 0.1 every 5 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)
model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=10)

def test(model, dataloaders):
  model.eval()
  accuracy = 0
  
  if torch.cuda.is_available():
    model.to(device='cuda')
  
    
  for images, labels in dataloaders['val']:
    images = Variable(images)
    labels = Variable(labels)
    if torch.cuda.is_available():
      images, labels = images.to(device='cuda'), labels.to(device='cuda')

      
    output = model.forward(images)
    ps = torch.exp(output)
    equality = (labels.data == ps.max(1)[1])
    accuracy += equality.type_as(torch.FloatTensor()).mean()
    testacc= accuracy/len(dataloaders['val'])
    #test_acc.append(testacc)
    
      
    print("Testing Accuracy: {:.3f}".format(testacc))

#test_acc=[]
test(model, dataloaders)

### Plotting the Accuracy and Losses for Training and Testing
#plt.plot(range(epochs), train_acc, 'b', label='Training Accu')
#plt.plot(range(epochs),  test_acc, 'r', label='Test Accu')
#plt.title('Training and Test Accuracy')
#plt.xlabel('Epochs')
#plt.ylabel('Accuracy')
#plt.legend()
#plt.show()

#train_accuracy_all = np.array(train_acc)
#test_accuracy_all = np.array(test_acc)
#train_loss_all = np.array(train_loss)
#test_loss_all = np.array(test_loss_all)

#np.save('train_accuracy.npy', train_accuracy_all)
#np.save('test_accuracy.npy', test_accuracy_all)
#np.save("train_loss.npy", train_loss_all)
#np.save("test_loss9.npy", test_loss_all)